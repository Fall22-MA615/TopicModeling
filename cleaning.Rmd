---
title: "Topic Modeling"
author: "Group 10"
date: "2022-11-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(tidyr)
library(DescTools)
library(tokenizers)
```

##Import Data
```{r cars}
imdb <- read_csv ("/Users/jiunlee/MSSP22/Fidelity/IMDB Dataset.csv", 
                    col_names = TRUE,
                    show_col_types = FALSE)
imdb <- imdb %>% select(-sentiment)

#remove duplicates of review
imdb <- unique(imdb)

## sample down

review_index <- 1:dim(imdb)[1]
text_df <- cbind(review_index,imdb)
text_df <- text_df[1:100,]
```

## Tokenizing, Removing stopwords
```{r pressure, echo=FALSE}

## create bigrams
token <- text_df %>%
  unnest_tokens(word, review)

## create a stop word vector
stop <-  unlist(stop_words[,1])

## drop the attribute
stop <- StripAttr(stop)

## add to the stop list
stop <- c(stop, "br")

## split the bigram list into two columns
check <-  token

## check both words individually agains stop word lists
remove <- check$word %in% stop

## to make it easier to see create a data frame
d <- cbind(token,remove)

## create an index of words(not stopwords)
f <- which(d$remove == FALSE)

clean_token <- d %>% slice(f) %>% select(-remove)

rm(check,d,token)
```
