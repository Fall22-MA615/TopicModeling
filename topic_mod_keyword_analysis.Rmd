---
title: "topic_mod"
author: 'Group 10: Bill Gao, Jiun Lee, Priam Vyas, Danya Zhang'
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(tidyr)
library(tidytext)
library(DescTools)
library(tokenizers)
library(readxl)
```

##Import Data
```{r cars}
imdb <- read_csv("/Users/priamvyas/Desktop/MSSP/615 Data Science in R/MidTerm Project/Topic Modelling/IMDB Dataset.csv", col_names = TRUE,
                  show_col_types = FALSE)
#imdb <- imdb %>% select(-sentiment)
imdb <- imdb[, 1]
#remove duplicates of review
imdb <- unique(imdb)
## sample down
review_index <- 1:dim(imdb)[1]
text_df <- cbind(review_index,imdb)
text_df <- text_df[1:100,]
```

## Tokenizing, Removing stopwords
```{r pressure, echo=FALSE}
library(stringr)
## tokenizing
token <- text_df %>%
  unnest_tokens(word, review) %>%
  count(review_index, word, sort=TRUE) %>%
  rename(count=n)
  
## create a stop word vector
stop <-  unlist(stop_words[,1])
## drop the attribute
stop <- StripAttr(stop)

check <-  token

## check words agains stop word lists
remove <- check$word %in% stop
## to make it easier to see create a data frame
d <- cbind(token,remove)
## create an index of words(not stopwords)
f <- which(d$remove == FALSE)
clean_token <- d %>% slice(f) %>% select(-remove)
rm(check,d,token)

#subset data frame that has no meaningless words
strings <- c("br","movie","film", "scene", "character","story","bit","lot","bad","act","hard","awful","good","plot","people", "cinema","audience","coburn","cast","heston","jimmy", "fred","worst","sir","time",'original','direction','version','1','2','3','4','5','6','7','8','9','10','effect','version','di','life','le','ryan','titta','henry','woman','idea','house','bela','simply','watch','friend','girl','tom','wife','real','hot','forget','feel','wait','make','pretty','earl','kane','rose','hitchcock','success','courtenay','script','guy','michael','suzanne','amir','ha')

meaningless <- str_detect(clean_token$word, paste(strings, collapse = "|"))
has_meaning <- which(meaningless==F)
clean_token <- clean_token %>% slice(has_meaning)

```
Here, we are cleaning the data and removing stop words that don't mean anything.

```{r}
#create lda model
library(topicmodels)

#convert sample token tibble to document term matrix for lda
clean_token_dmat <- clean_token %>%
  cast_dtm(review_index, word, count)

#select k=8 because 8 general film genres
imdb_lda <- LDA(clean_token_dmat, k = 6, control = list(seed = 1234))
imdb_topics <- tidy(imdb_lda, matrix = "beta")
```


```{r}
library(ggplot2)
imdb_top_terms <- imdb_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 6) %>% 
  ungroup() %>%
  arrange(topic, -beta)

imdb_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
The plot above shows the top 6 words for each topic in our LDA. We've split up our LDA into 6 genres, which represents the number of topics we have.


```{r}
#gamma: per-document-per-topic probabilities
imdb_documents <- tidy(imdb_lda, matrix = "gamma")
imdb_documents

#most common words in document
imdb_keywords <- tidy(clean_token_dmat) %>%
  filter(document == 6) %>%
  arrange(desc(count))
imdb_keywords

assignments <- augment(imdb_lda, data = clean_token_dmat)
assignments
```
The assignments tibble above count up the words for each topic. 

```{r}

imdb_documents$document <- as.integer(as.character(imdb_documents$document))

lda_gamma <- full_join(imdb_documents, clean_token, by = c("document" = "review_index"))
lda_gamma

top_words <- lda_gamma %>% 
  filter(gamma > 0.9) %>% 
  drop_na() %>% 
  count(topic, word, sort = TRUE)

top_words

#Found the most frequent words used in the documents. Used the gamma value to get the top 10% of the most occuring words in the document.

```
```{r}
top_words %>%
  group_by(topic) %>%
  slice_max(n, n = 5, with_ties = FALSE) %>%
  ungroup %>%
  mutate(word = reorder_within(word, n, topic)) %>%
  ggplot(aes(n, word, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Top keywords for each LDA topic",
       x = "Number of documents", y = NULL) +
  scale_y_reordered() +
  facet_wrap(~ topic, ncol = 4, scales = "free")

#Graphed

```


